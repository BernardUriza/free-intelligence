TRANSCRIPTION PIPELINE PERFORMANCE BOTTLENECKS - QUICK SUMMARY
================================================================

EXECUTIVE FINDINGS:
- LLM speaker classification is the PRIMARY bottleneck (63% of time)
- Whisper transcription is secondary (27% of time)  
- 10-minute audio takes ~3-4 minutes with LLM, ~30 seconds without
- V2 pipeline + low-priority worker (already default) is well-optimized

RANKING OF BOTTLENECKS:

1. LLM SPEAKER CLASSIFICATION (CRITICAL - 63% of time)
   - 8-12 seconds per segment
   - Serialized (one at a time)
   - File: backend/diarization_service.py::classify_speaker_with_llm()
   - Quick fix: Set ENABLE_LLM_CLASSIFICATION=false → 3-4x speedup
   - Line reference: diarization_service.py:176-259

2. WHISPER TRANSCRIPTION (MAJOR - 27% of time)
   - Performance by model (30s audio on CPU):
     * tiny:    0.3s (10x faster than small)
     * base:    0.6s (5x faster)
     * small:   1.0s (DEFAULT)
     * medium:  2.5s
     * large-v3: 5.0s
   - File: backend/whisper_service.py
   - Quick fix: Set WHISPER_MODEL_SIZE=base → 2-3x speedup
   - Line reference: whisper_service.py:25

3. I/O OPERATIONS (MODERATE - 13% of time)
   - ffmpeg chunk extraction: 2-5s per chunk
   - HDF5 writes: 0.5-1s per chunk
   - File: backend/diarization_service.py::extract_chunk()
   - Line reference: diarization_service.py:147-173

4. PARALLELIZATION OVERHEAD (MINOR - 7% of time)
   - Context switching, lock contention
   - Already well-optimized in V2
   - File: backend/diarization_service_v2.py
   - Line reference: diarization_service_v2.py:224-254

CURRENT CONFIGURATION LOCATIONS:

Model Size:
  - whisper_service.py:25 (WHISPER_MODEL_SIZE)
  - diarization_service.py:31 (default is small)
  - diarization_worker_lowprio.py:40

Compute Type:
  - whisper_service.py:26 (WHISPER_COMPUTE_TYPE, default: int8)
  - diarization_worker_lowprio.py:42

LLM Classification:
  - diarization_service.py:40 (ENABLE_LLM_CLASSIFICATION)
  - diarization_service.py:39 (FI_ENRICHMENT)
  - diarization_service.py:34 (OLLAMA_BASE_URL)
  - diarization_service.py:35 (OLLAMA_MODEL)

Chunking:
  - diarization_service.py:31 (DIARIZATION_CHUNK_SEC, default: 30)
  - diarization_service_v2.py:52 (DIARIZATION_PARALLEL_CHUNKS, default: 2)

QUICK WINS (No code changes):

1. Disable LLM: ENABLE_LLM_CLASSIFICATION=false
   Impact: 3-4x faster (60-70% time saved)

2. Smaller model: WHISPER_MODEL_SIZE=base (dev) or tiny (ultra-fast)
   Impact: 2-3x faster (27% time saved)

3. Increase chunks: DIARIZATION_CHUNK_SEC=45 (vs 30)
   Impact: 25% faster (fewer LLM calls)

4. Reduce beam: ASR_BEAM=1 (currently hardcoded as 5)
   Impact: 10-15% faster (needs code change at whisper_service.py:188)

PERFORMANCE TARGETS:

Development (tiny model, no LLM):
- 10 min audio: 15-20 seconds

Production (small model, with LLM):
- 10 min audio: 3-4 minutes
- 30 min audio: 9-12 minutes

GPU (if available):
- 10 min audio: 30-60 seconds

FILE SIZE THRESHOLDS:

<1 min:    Fast (instant, cold-start dominated)
1-5 min:   Fast (10-30 seconds)
5-15 min:  Moderate (1-5 minutes)
15-60 min: Slow (5-20 minutes) → Use low-priority worker
>60 min:   Very slow (20+ minutes) → Batch process

HARDCODED VALUES TO EXPOSE:

1. Beam size (whisper_service.py:188)
   Current: beam_size=5
   Should be: int(os.getenv("ASR_BEAM", "5"))

2. Beam size in worker (diarization_worker_lowprio.py:300)
   Same issue as above

FULL ANALYSIS:

See: docs/TRANSCRIPTION_PERFORMANCE_ANALYSIS.md (825 lines)
Covers:
- Detailed architecture breakdown
- Performance impact quantification
- Optimization opportunities (quick, medium, long-term)
- Config recommendations (dev, prod, GPU)
- Code locations for tuning
- Expected performance metrics

FILES TO MONITOR:

Primary:
- backend/whisper_service.py (transcription, model loading)
- backend/api/transcribe.py (transcription endpoint)
- backend/diarization_service_v2.py (diarization V2 pipeline)
- backend/diarization_service.py (speaker classification)
- backend/api/diarization.py (diarization endpoint)
- backend/diarization_worker_lowprio.py (low-priority worker, HDF5)

Secondary:
- backend/audio_storage.py (file I/O)
- storage/diarization.h5 (incremental results)
- storage/audio/{session_id}/*.{ext} (audio files)

Generated: 2025-10-31
