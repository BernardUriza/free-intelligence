# LLM Router Policy - Free Intelligence

**Fecha**: 2025-10-25
**Task**: FI-CORE-FIX-001
**Sprint**: SPR-2025W44 (Sprint 2)
**Autor**: Bernard Uriza Orozco

---

## ğŸ¯ PropÃ³sito

**TODA** llamada a LLM debe pasar por router centralizado. Sin excepciones.

Esta polÃ­tica garantiza:
- **Control centralizado**: Un solo punto de entrada para LLM calls
- **Logging automÃ¡tico**: El router maneja audit logs
- **Rate limiting**: FÃ¡cil agregar throttling/quotas
- **Model switching**: Cambiar providers sin tocar cÃ³digo
- **Cost tracking**: Monitoreo centralizado de uso

---

## ğŸ“‹ PolÃ­tica

### Reglas Obligatorias

1. âŒ **PROHIBIDO**: Imports directos de librerÃ­as LLM
   ```python
   # âŒ PROHIBIDO
   import anthropic
   from openai import OpenAI
   import cohere
   ```

2. âŒ **PROHIBIDO**: Llamadas directas a APIs
   ```python
   # âŒ PROHIBIDO
   client.messages.create(...)
   openai.ChatCompletion.create(...)
   cohere.generate(...)
   ```

3. âœ… **OBLIGATORIO**: Uso de router centralizado
   ```python
   # âœ… PERMITIDO
   from llm_router import route_llm_call
   response = route_llm_call(prompt=..., model=...)
   ```

---

## ğŸ”§ ImplementaciÃ³n

### Router Centralizado (Futuro)

El mÃ³dulo `llm_router.py` (a implementar en FI-CORE-FEAT-006) manejarÃ¡:

```python
# backend/llm_router.py (futuro)
from llm_audit_policy import require_audit_log
from audit_logs import append_audit_log

@require_audit_log
def route_llm_call(
    prompt: str,
    model: str = "claude-3-5-sonnet-20241022",
    user_id: str = None,
    **kwargs
) -> str:
    """
    Router centralizado para TODAS las llamadas a LLM.

    Features:
    - Audit logging automÃ¡tico
    - Rate limiting
    - Model switching
    - Cost tracking
    - Error handling
    """
    # 1. Validar inputs
    # 2. Seleccionar provider (anthropic, openai, etc.)
    # 3. Ejecutar llamada
    # 4. Registrar en audit_logs
    # 5. Retornar resultado

    # AUDIT LOG (automÃ¡tico)
    append_audit_log(
        operation="LLM_CALL_ROUTED",
        user_id=user_id,
        endpoint="route_llm_call",
        payload=prompt,
        result=response,
        status="SUCCESS",
        metadata={"model": model, "tokens": token_count}
    )

    return response
```

### Uso en AplicaciÃ³n

```python
# âœ… CORRECTO: Via router
from llm_router import route_llm_call

def generate_response(user_prompt: str, user_id: str) -> str:
    """
    Genera respuesta usando LLM via router.

    No necesita @require_audit_log porque el router
    ya lo maneja automÃ¡ticamente.
    """
    response = route_llm_call(
        prompt=user_prompt,
        model="claude-3-5-sonnet-20241022",
        user_id=user_id
    )
    return response
```

---

## ğŸš« LibrerÃ­as Prohibidas

El validador detecta automÃ¡ticamente imports de:

```python
FORBIDDEN_IMPORTS = {
    'anthropic',           # Anthropic Claude
    'openai',              # OpenAI GPT
    'cohere',              # Cohere
    'google.generativeai', # Google Gemini
    'huggingface_hub',     # HuggingFace
    'transformers',        # HuggingFace Transformers
}
```

### Llamadas Prohibidas

```python
FORBIDDEN_CALL_PATTERNS = {
    'messages.create',           # anthropic
    'chat.completions.create',   # openai
    'ChatCompletion.create',     # openai legacy
    'Completion.create',         # openai legacy
    'generate',                  # cohere, huggingface
    'generate_content',          # google
}
```

---

## ğŸ” ValidaciÃ³n

### ValidaciÃ³n EstÃ¡tica (AST-based)

```bash
# Escanear directorio en busca de violaciones
python3 backend/llm_router_policy.py scan backend/

# Validar codebase (retorna exit code 0/1)
python3 backend/llm_router_policy.py validate backend/
```

**Output esperado** (sin violaciones):

```
âœ… ROUTER POLICY VALIDATION PASSED
   No direct LLM API calls detected
   All LLM interactions must use centralized router
```

**Output con violaciones**:

```
âŒ ROUTER POLICY VIOLATIONS DETECTED

ğŸ“ backend/llm_client.py
   ğŸš« Forbidden Imports:
      â€¢ Direct import of LLM library: 'anthropic'

   ğŸš« Direct API Calls:
      â€¢ Line 45: Direct LLM API call: 'messages.create'

Total violations: 2

Policy: ALL LLM calls must:
  1. Use centralized router (llm_router module)
  2. NO direct imports of anthropic, openai, etc.
  3. NO direct API calls (messages.create, etc.)
```

### Pre-commit Hook (Futuro)

En **FI-CICD-FEAT-001** se integrarÃ¡ en pipeline:

```yaml
# .pre-commit-config.yaml
- repo: local
  hooks:
    - id: llm-router-validation
      name: LLM Router Policy Validator
      entry: python3 backend/llm_router_policy.py validate backend/
      language: system
      pass_filenames: false
```

---

## ğŸ¨ Patrones de Uso

### âŒ BAD: Import y llamada directa

```python
import anthropic

def call_claude(prompt):
    # âŒ Import directo + llamada directa
    client = anthropic.Anthropic(api_key="...")
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.content[0].text
```

### âŒ BAD: Solo import directo

```python
from openai import OpenAI

def call_openai(prompt):
    # âŒ Aunque uses router despuÃ©s, el import estÃ¡ prohibido
    client = OpenAI()
    return client.chat.completions.create(...)
```

### âœ… GOOD: Via router

```python
from llm_router import route_llm_call

def generate_text(prompt, user_id):
    # âœ… Router centralizado
    # Audit logging automÃ¡tico
    # Rate limiting automÃ¡tico
    # Cost tracking automÃ¡tico
    return route_llm_call(
        prompt=prompt,
        model="claude-3-5-sonnet-20241022",
        user_id=user_id
    )
```

### âœ… GOOD: Con configuraciÃ³n

```python
from llm_router import route_llm_call, LLMConfig

def generate_with_config(prompt, user_id):
    # âœ… Con configuraciÃ³n avanzada
    config = LLMConfig(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1000,
        temperature=0.7
    )

    return route_llm_call(
        prompt=prompt,
        user_id=user_id,
        config=config
    )
```

---

## ğŸ§ª Tests

### Cobertura

- âœ… DetecciÃ³n de imports prohibidos (anthropic, openai, etc.)
- âœ… DetecciÃ³n de llamadas directas a APIs
- âœ… ExtracciÃ³n de imports con `from` y `import`
- âœ… ExtracciÃ³n de attribute calls anidados
- âœ… Escaneo de archivos individuales
- âœ… Escaneo recursivo de directorios
- âœ… ValidaciÃ³n de codebase completo
- âœ… Manejo de syntax errors

**Total**: 27 tests passing (0.006s)

```bash
# Ejecutar tests
python3 -m unittest tests.test_llm_router_policy -v
```

---

## ğŸ“Š Beneficios del Router

### 1. Control Centralizado

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Application Code                   â”‚
â”‚  â”œâ”€ Feature A: route_llm_call()     â”‚
â”‚  â”œâ”€ Feature B: route_llm_call()     â”‚
â”‚  â””â”€ Feature C: route_llm_call()     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Router (single entry point)    â”‚
â”‚  â”œâ”€ Audit logging                   â”‚
â”‚  â”œâ”€ Rate limiting                   â”‚
â”‚  â”œâ”€ Cost tracking                   â”‚
â”‚  â”œâ”€ Error handling                  â”‚
â”‚  â””â”€ Model switching                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LLM Providers                      â”‚
â”‚  â”œâ”€ Anthropic (Claude)              â”‚
â”‚  â”œâ”€ OpenAI (GPT)                    â”‚
â”‚  â””â”€ Cohere                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Cambio de Provider sin CÃ³digo

```python
# Cambiar de Claude a GPT sin tocar cÃ³digo de aplicaciÃ³n
# Solo modificar router:

# backend/llm_router.py
def route_llm_call(prompt, model="gpt-4", **kwargs):
    # Cambio automÃ¡tico sin refactor
    if model.startswith("claude"):
        return _call_anthropic(prompt, model, **kwargs)
    elif model.startswith("gpt"):
        return _call_openai(prompt, model, **kwargs)
```

### 3. Audit Trail AutomÃ¡tico

```python
# ANTES (sin router): Cada funciÃ³n debe hacer append_audit_log()
@require_audit_log
def call_claude(prompt):
    response = client.messages.create(...)
    append_audit_log(...)  # âŒ FÃ¡cil olvidar
    return response

# DESPUÃ‰S (con router): Router lo hace automÃ¡tico
def generate_text(prompt):
    return route_llm_call(prompt=prompt)  # âœ… Audit automÃ¡tico
```

### 4. Rate Limiting Global

```python
# Router puede implementar throttling global
def route_llm_call(prompt, model, user_id, **kwargs):
    # Check rate limit
    if not rate_limiter.check(user_id):
        raise RateLimitExceeded(f"User {user_id} exceeded quota")

    # Proceed with call
    return _execute_llm_call(...)
```

---

## ğŸ“ˆ Roadmap

### Fase 1 (Actual): DetecciÃ³n EstÃ¡tica âœ…

- Validador AST para imports y calls
- CLI para escaneo de cÃ³digo
- Tests completos
- DocumentaciÃ³n

### Fase 2 (Futuro): ImplementaciÃ³n de Router

- MÃ³dulo `llm_router.py` (FI-CORE-FEAT-006)
- Support para Anthropic, OpenAI, Cohere
- Audit logging automÃ¡tico
- Rate limiting bÃ¡sico

### Fase 3 (Futuro): Features Avanzadas

- Cost tracking en tiempo real
- Model fallback automÃ¡tico
- Caching de responses
- Streaming support
- Retry logic con backoff

---

## ğŸ”— Referencias

- **LLM Audit Policy**: `docs/llm-audit-policy.md` (FI-CORE-FEAT-004)
- **Audit Logs**: `backend/audit_logs.py` (FI-SEC-FEAT-003)
- **Event Validator**: `backend/event_validator.py` (FI-API-FEAT-001)

---

## âœ… Status Actual

- **ImplementaciÃ³n**: Validador completo âœ…
- **Tests**: 27/27 passing âœ…
- **DocumentaciÃ³n**: Completa âœ…
- **ValidaciÃ³n**: Backend actual 0 violaciones âœ…

**PrÃ³ximo paso**: Implementar `llm_router.py` en FI-CORE-FEAT-006 (Sprint futuro)

**Nota**: Por ahora el backend NO tiene llamadas a LLM (infraestructura pura), por lo que la validaciÃ³n pasa. Cuando se implemente FI-CORE-FEAT-001 (Middleware LLM), DEBE usar el router.
