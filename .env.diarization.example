# Free Intelligence - Diarization Configuration
# Copy to .env.local and customize for your environment

# ============================================================================
# Whisper ASR Configuration
# ============================================================================

# Whisper model size (impacts speed vs accuracy)
# Options: tiny, base, small, medium, large-v3
# Recommendations:
#   - CPU development: tiny or base (fast, lower accuracy)
#   - CPU production: small (BEST for DS923+: 2-3x faster than large-v3)
#   - GPU production: large-v3 (best accuracy, slower)
WHISPER_MODEL_SIZE=small

# Whisper compute type (CPU optimization)
# Options: int8, float16, float32
# int8 = fastest on CPU (50-70% faster than float16)
WHISPER_COMPUTE_TYPE=int8

# Whisper CPU optimization
# DS923+ (Ryzen R1600, 4 threads): use cpu_threads=3, num_workers=1
ASR_CPU_THREADS=3
ASR_NUM_WORKERS=1
ASR_BEAM=1

# VAD (Voice Activity Detection) - cuts silence, reduces processing time
ASR_VAD=true

# Chunk duration in seconds (how long each audio segment to transcribe)
# Smaller = more API calls but better segmentation
# Larger = fewer calls but may miss speaker changes (20‚Üí30 for less overhead)
DIARIZATION_CHUNK_SEC=30

# Minimum segment duration (filter out very short segments)
MIN_SEGMENT_SEC=0.5

# ============================================================================
# LLM Speaker Classification (Ollama)
# ============================================================================

# Enable/disable LLM-based speaker classification
# Set to "false" to skip LLM and label all speakers as DESCONOCIDO
# This is useful if:
#   - You don't have Ollama installed
#   - You want faster processing (transcription-only)
#   - You're testing/debugging
ENABLE_LLM_CLASSIFICATION=true

# Ollama server URL
# Default: http://localhost:11434
# Change if Ollama is running on a different machine/port
LLM_BASE_URL=http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model for speaker classification
# Recommended: qwen2.5:7b-instruct-q4_0 (good balance)
# Lighter: qwen2.5:3b-instruct (faster, less accurate)
# Heavier: qwen2.5:14b-instruct (more accurate, slower)
LLM_MODEL=qwen2.5:7b-instruct-q4_0
DIARIZATION_LLM_MODEL=qwen2.5:7b-instruct-q4_0

# LLM request timeout in milliseconds (30s = 30000ms, 60s = 60000ms)
# Increase if you get timeout errors on CPU
# Decrease for faster failure detection
LLM_TIMEOUT_MS=60000

# Legacy env var for backward compatibility (deprecated, use LLM_TIMEOUT_MS)
DIARIZATION_LLM_TIMEOUT=60

# FI_ENRICHMENT kill switch (on/off)
# Set to "off" to skip LLM enrichment entirely (same as ENABLE_LLM_CLASSIFICATION=false)
FI_ENRICHMENT=on

# LLM temperature (0.0-1.0)
# Lower = more deterministic
# Higher = more creative (not recommended for classification)
DIARIZATION_LLM_TEMP=0.1

# ============================================================================
# Performance Tuning Recommendations
# ============================================================================

# üñ•Ô∏è CPU Development (fast iteration):
#   WHISPER_MODEL_SIZE=tiny
#   WHISPER_COMPUTE_TYPE=int8
#   ASR_CPU_THREADS=3
#   ASR_NUM_WORKERS=1
#   ASR_BEAM=1
#   ASR_VAD=true
#   ENABLE_LLM_CLASSIFICATION=false
#   FI_ENRICHMENT=off
#   DIARIZATION_CHUNK_SEC=30

# üñ•Ô∏è CPU Production (DS923+ recommended):
#   WHISPER_MODEL_SIZE=small
#   WHISPER_COMPUTE_TYPE=int8
#   ASR_CPU_THREADS=3
#   ASR_NUM_WORKERS=1
#   ASR_BEAM=1
#   ASR_VAD=true
#   ENABLE_LLM_CLASSIFICATION=true
#   FI_ENRICHMENT=on
#   LLM_TIMEOUT_MS=60000
#   DIARIZATION_CHUNK_SEC=30

# üöÄ GPU Production (best quality):
#   WHISPER_MODEL_SIZE=large-v3
#   WHISPER_COMPUTE_TYPE=float16
#   ASR_CPU_THREADS=4
#   ASR_NUM_WORKERS=2
#   ASR_BEAM=5
#   ASR_VAD=true
#   ENABLE_LLM_CLASSIFICATION=true
#   FI_ENRICHMENT=on
#   LLM_TIMEOUT_MS=15000
#   DIARIZATION_CHUNK_SEC=15

# ============================================================================
# Troubleshooting
# ============================================================================

# ‚ùå Problem: "LLM_TIMEOUT" or "OLLAMA_UNREACHABLE" errors
# ‚úÖ Solutions:
#    1. Check Ollama is running: `ollama list`
#    2. Increase timeout: DIARIZATION_LLM_TIMEOUT=60
#    3. Disable LLM: ENABLE_LLM_CLASSIFICATION=false

# ‚ùå Problem: Slow transcription (large-v3 on CPU)
# ‚úÖ Solutions:
#    1. Use smaller model: WHISPER_MODEL_SIZE=small
#    2. Increase chunk size: DIARIZATION_CHUNK_SEC=30
#    3. Run on GPU (if available)

# ‚ùå Problem: Poor speaker classification
# ‚úÖ Solutions:
#    1. Use larger LLM: DIARIZATION_LLM_MODEL=qwen2.5:14b-instruct
#    2. Decrease chunk size: DIARIZATION_CHUNK_SEC=15
#    3. Check Ollama model is downloaded: `ollama pull qwen2.5:7b-instruct-q4_0`
